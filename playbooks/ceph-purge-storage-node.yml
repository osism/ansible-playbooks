---
# This playbook purges a single storage node
#
# It removes: containers, configuration files and ALL THE DATA
#
# The individual OSDs must first be removed from the Ceph cluster.
# The services must be stopped.

- name: confirm whether user really meant to purge the node

  hosts: localhost

  gather_facts: false

  vars_prompt:
    - name: ireallymeanit
      prompt: >
        Are you sure you want to purge the cluster?
        Note that if with_pkg is not set docker packages
        and more will be uninstalled from non-atomic hosts.
        Do you want to continue?
      default: 'no'
      private: false

  tasks:

    - name: exit playbook, if list of hosts contains more than one node
      fail:
        msg: >
          "Exiting purge-storage-node playbook, node was NOT purged.
          This playbook can only be used with a single node."
        when: play_hosts|length > 0

    - name: exit playbook, if user did not mean to purge node
      fail:
        msg: >
          "Exiting purge-storage-node playbook, node was NOT purged.
          To purge the node, either say 'yes' on the prompt or
          or use `-e ireallymeanit=yes` on the command line when
          invoking the playbook"
      when: ireallymeanit != 'yes'

    - name: set ceph_docker_registry value if not set
      set_fact:
        ceph_docker_registry: "docker.io"
      when: ceph_docker_registry is not defined

- name: purge ceph osd node

  hosts: "{{ osd_group_name | default('osds') }}"

  gather_facts: true
  become: true

  tasks:

    - name: exit playbook, if list of hosts contains more than one node
      fail:
        msg: >
          "Exiting purge-storage-node playbook, node was NOT purged.
          This playbook can only be used with a single node."
        when: play_hosts|length > 0

    - import_role:
        name: ceph-defaults

    - name: get all the running osds
      shell: |
        systemctl list-units --all | grep -oE "ceph-osd@([0-9]+).service"
      register: osd_units
      ignore_errors: true

    - name: disable ceph osd service
      service:
        name: "{{ item }}"
        state: stopped
        enabled: false
      with_items: "{{ osd_units.stdout_lines }}"

    - name: remove osd mountpoint tree
      file:
        path: /var/lib/ceph/osd/
        state: absent
      ignore_errors: true

    - name: default lvm_volumes if not defined
      set_fact:
        lvm_volumes: []
      when: lvm_volumes is not defined

    - name: zap and destroy osds created by ceph-volume with lvm_volumes
      ceph_volume:
        data: "{{ item.data }}"
        data_vg: "{{ item.data_vg|default(omit) }}"
        journal: "{{ item.journal|default(omit) }}"
        journal_vg: "{{ item.journal_vg|default(omit) }}"
        db: "{{ item.db|default(omit) }}"
        db_vg: "{{ item.db_vg|default(omit) }}"
        wal: "{{ item.wal|default(omit) }}"
        wal_vg: "{{ item.wal_vg|default(omit) }}"
        action: "zap"
      environment:
        CEPH_VOLUME_DEBUG: 1
        CEPH_CONTAINER_IMAGE: "{{ ceph_docker_registry }}/{{ ceph_docker_image }}:{{ ceph_docker_image_tag }}"
        CEPH_CONTAINER_BINARY: "{{ container_binary }}"
      with_items: "{{ lvm_volumes }}"
      when: lvm_volumes | default([]) | length > 0

    - name: zap and destroy osds created by ceph-volume with devices
      ceph_volume:
        data: "{{ item }}"
        action: "zap"
      environment:
        CEPH_VOLUME_DEBUG: 1
        CEPH_CONTAINER_IMAGE: "{{ ceph_docker_registry }}/{{ ceph_docker_image }}:{{ ceph_docker_image_tag }}"
        CEPH_CONTAINER_BINARY: "{{ container_binary }}"
      with_items: "{{ devices | default([]) }}"
      when: devices | default([]) | length > 0

    - name: remove ceph osd service
      file:
        path: /etc/systemd/system/ceph-osd@.service
        state: absent

    - name: include vars from group_vars/osds.yml
      include_vars:
        file: "{{ item }}"
      with_first_found:
        - files:
            - "{{ playbook_dir }}/group_vars/osds"
            - "{{ playbook_dir }}/group_vars/osds.yml"
          skip: true

    - name: find all osd_disk_prepare logs
      find:
        paths: "{{ ceph_osd_docker_run_script_path | default('/usr/share') }}"
        pattern: "ceph-osd-prepare-*.log"
      register: osd_disk_prepare_logs

    - name: ensure all osd_disk_prepare logs are removed
      file:
        path: "{{ item.path }}"
        state: absent
      with_items: "{{ osd_disk_prepare_logs.files }}"


- name: purge ceph directories

  hosts: "{{ osd_group_name | default('osds') }}"

  gather_facts: false  # Already gathered previously
  become: true

  tasks:

    - name: exit playbook, if list of hosts contains more than one node
      fail:
        msg: >
          "Exiting purge-storage-node playbook, node was NOT purged.
          This playbook can only be used with a single node."
        when: play_hosts|length > 0

    - name: purge ceph directories for "{{ ansible_hostname }}"
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /etc/ceph
        - /var/log/ceph

    - name: remove ceph data
      shell: rm -rf /var/lib/ceph/*
